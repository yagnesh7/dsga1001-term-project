{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/combined_data_v3.csv')\n",
    "data = data.drop_duplicates().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>headline</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>177592</th>\n",
       "      <td>ABC Australia</td>\n",
       "      <td>barcelona terror cell dismantled police hunt s...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167905</th>\n",
       "      <td>ABC Australia</td>\n",
       "      <td>nrn mg apology</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-06-08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337551</th>\n",
       "      <td>atlantic</td>\n",
       "      <td>Americans Are Pretty Skeptical That Hard Work ...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-04-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56740</th>\n",
       "      <td>Huffington Post</td>\n",
       "      <td>'Moonlight,' 'Jackie' And 'Manchester By The S...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-11-22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64751</th>\n",
       "      <td>Huffington Post</td>\n",
       "      <td>UN/OPCW Report Blames Syria Government, Islami...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-08-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145337</th>\n",
       "      <td>ABC Australia</td>\n",
       "      <td>shelburne bay handed back to the wuthathi people</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-12-15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334420</th>\n",
       "      <td>atlantic</td>\n",
       "      <td>How the White House's Immigration Reforms Migh...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-08-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13342</th>\n",
       "      <td>Huffington Post</td>\n",
       "      <td>doing academic time</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180096</th>\n",
       "      <td>ABC Australia</td>\n",
       "      <td>western queensland woman plans endurance horse...</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017-09-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82779</th>\n",
       "      <td>Huffington Post</td>\n",
       "      <td>This 'Fantastic Beasts And Where To Find Them'...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-01-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 source                                           headline  \\\n",
       "177592    ABC Australia  barcelona terror cell dismantled police hunt s...   \n",
       "167905    ABC Australia                                     nrn mg apology   \n",
       "337551         atlantic  Americans Are Pretty Skeptical That Hard Work ...   \n",
       "56740   Huffington Post  'Moonlight,' 'Jackie' And 'Manchester By The S...   \n",
       "64751   Huffington Post  UN/OPCW Report Blames Syria Government, Islami...   \n",
       "145337    ABC Australia   shelburne bay handed back to the wuthathi people   \n",
       "334420         atlantic  How the White House's Immigration Reforms Migh...   \n",
       "13342   Huffington Post                                doing academic time   \n",
       "180096    ABC Australia  western queensland woman plans endurance horse...   \n",
       "82779   Huffington Post  This 'Fantastic Beasts And Where To Find Them'...   \n",
       "\n",
       "        year        date  is_sarcastic  \n",
       "177592  2017  2017-08-19             0  \n",
       "167905  2017  2017-06-08             0  \n",
       "337551  2017  2017-04-06             0  \n",
       "56740   2016  2016-11-22             0  \n",
       "64751   2016  2016-08-24             0  \n",
       "145337  2016  2016-12-15             0  \n",
       "334420  2017  2017-08-07             0  \n",
       "13342   2019  2019-01-01             0  \n",
       "180096  2017  2017-09-06             0  \n",
       "82779   2016  2016-01-31             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps\n",
    "1. Standardize common abbreviations* (WIP)\n",
    "    1. u.s. --> usa\n",
    "1. Lowercase\n",
    "1. Expand Contractions\n",
    "1. Optional:\n",
    "    1. Remove Source Specific Language\n",
    "    1. Remove Profanity\n",
    "1. Remove Special Characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words(text, replace_dict):\n",
    "    tokens = []\n",
    "    for w in text.split():\n",
    "        word = w\n",
    "        for t in replace_dict.keys():\n",
    "            if w == t:\n",
    "                word = replace_dict[t]\n",
    "        tokens.append(word)\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean'] = data['headline']\n",
    "\n",
    "## Standardize Common Abbreviations\n",
    "translate_dict = {\n",
    "    \"US\": \"USA\",\n",
    "    \"U.S.\": \"USA\",\n",
    "    \"u.s.\": \"USA\",\n",
    "    \"u.s\": \"USA\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "data['clean'] = data['clean'].apply(lambda x: replace_words(x, translate_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "data['clean'] = data['clean'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand Contractions\n",
    "contractions_dict = { \"ain't\": \"are not\",\"aren't\": \"are not\",\n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
    "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who's\": \"who is\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\"}\n",
    "\n",
    "\n",
    "\n",
    "data['clean'] = data['clean'].apply(lambda x: replace_words(x, contractions_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21 people found out today they will not die in prison'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example should read: who is afraid\n",
    "data['clean'][77627]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Special Characters\n",
    "data['clean'] = data['clean'].apply(lambda x: re.sub('[^A-Za-z0-9 ]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         former versace store clerk sues over secret bl...\n",
       "1         the roseanne revival catches up to our thorny ...\n",
       "2         jk rowling wishes snape happy birthday in the ...\n",
       "3                                advancing the worlds women\n",
       "4             the fascinating case for eating labgrown meat\n",
       "                                ...                        \n",
       "374287    overworked gamer wishes he could just stay hom...\n",
       "374288    how king henry viii became the worlds first ga...\n",
       "374289    ring fit user hates how crowded adventure gets...\n",
       "374290    gamer postpones new years resolution until q3 ...\n",
       "374291    netflix announces its losing the audio portion...\n",
       "Name: clean, Length: 374292, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['source', 'year', 'clean', 'is_sarcastic']].to_csv('../data/combined_clean.csv', index = False, sep = \"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
