{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(439412, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/combined_data_v6.csv')\n",
    "data['headline'] = data['headline'].astype('U')\n",
    "data['year'] = data['year'].astype(int)\n",
    "data = data.drop_duplicates().reset_index(drop = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>headline</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131565</th>\n",
       "      <td>ABC Australia</td>\n",
       "      <td>cam mccarthy afl trade talk between fremantle ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-09-20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141073</th>\n",
       "      <td>ABC Australia</td>\n",
       "      <td>iraqi forces find australians is flag in mosul</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-11-18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82365</th>\n",
       "      <td>Huffington Post</td>\n",
       "      <td>Inmates In Flint Forced To Consume And Use Con...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-02-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198663</th>\n",
       "      <td>ABC Australia</td>\n",
       "      <td>old boys urge anglican church to hand over sch...</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-02-09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213062</th>\n",
       "      <td>ABC Australia</td>\n",
       "      <td>father murdered son to avoid missing afl grand...</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-06-11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91820</th>\n",
       "      <td>ABC Australia</td>\n",
       "      <td>greyhound racing national rallies call for end...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-02-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298492</th>\n",
       "      <td>npr</td>\n",
       "      <td>Wash. Investigates More Possible Coronavirus C...</td>\n",
       "      <td>2020</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334091</th>\n",
       "      <td>guardian</td>\n",
       "      <td>Rhode Island governor sees off leftwing challe...</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-09-13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278294</th>\n",
       "      <td>npr</td>\n",
       "      <td>'How To Think Like An Anthropologist' — And Wh...</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-02-13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278432</th>\n",
       "      <td>npr</td>\n",
       "      <td>French Food Waste Law Changing How Grocery Sto...</td>\n",
       "      <td>2018</td>\n",
       "      <td>2018-02-24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 source                                           headline  \\\n",
       "131565    ABC Australia  cam mccarthy afl trade talk between fremantle ...   \n",
       "141073    ABC Australia     iraqi forces find australians is flag in mosul   \n",
       "82365   Huffington Post  Inmates In Flint Forced To Consume And Use Con...   \n",
       "198663    ABC Australia  old boys urge anglican church to hand over sch...   \n",
       "213062    ABC Australia  father murdered son to avoid missing afl grand...   \n",
       "91820     ABC Australia  greyhound racing national rallies call for end...   \n",
       "298492              npr  Wash. Investigates More Possible Coronavirus C...   \n",
       "334091         guardian  Rhode Island governor sees off leftwing challe...   \n",
       "278294              npr  'How To Think Like An Anthropologist' — And Wh...   \n",
       "278432              npr  French Food Waste Law Changing How Grocery Sto...   \n",
       "\n",
       "        year        date  is_sarcastic  \n",
       "131565  2016  2016-09-20             0  \n",
       "141073  2016  2016-11-18             0  \n",
       "82365   2016  2016-02-05             0  \n",
       "198663  2018  2018-02-09             0  \n",
       "213062  2018  2018-06-11             0  \n",
       "91820   2016  2016-02-07             0  \n",
       "298492  2020  2020-03-01             0  \n",
       "334091  2018  2018-09-13             0  \n",
       "278294  2018  2018-02-13             0  \n",
       "278432  2018  2018-02-24             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps\n",
    "1. Standardize common abbreviations* (WIP)\n",
    "    1. u.s. --> usa\n",
    "1. Lowercase\n",
    "1. Expand Contractions\n",
    "1. Optional:\n",
    "    1. Remove Source Specific Language\n",
    "    1. Remove Profanity\n",
    "1. Remove Special Characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words(text, replace_dict):\n",
    "    tokens = []\n",
    "    for w in text.split():\n",
    "        word = w\n",
    "        for t in replace_dict.keys():\n",
    "            if w == t:\n",
    "                word = replace_dict[t]\n",
    "        tokens.append(word)\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean'] = data['headline']\n",
    "\n",
    "## Standardize Common Abbreviations\n",
    "translate_dict = {\n",
    "    \"US\": \"USA\",\n",
    "    \"U.S.\": \"USA\",\n",
    "    \"u.s.\": \"USA\",\n",
    "    \"u.s\": \"USA\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "data['clean'] = data['clean'].apply(lambda x: replace_words(x, translate_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "data['clean'] = data['clean'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand Contractions\n",
    "contractions_dict = { \"ain't\": \"are not\",\"aren't\": \"are not\",\n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
    "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who's\": \"who is\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\"}\n",
    "\n",
    "\n",
    "\n",
    "data['clean'] = data['clean'].apply(lambda x: replace_words(x, contractions_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>6 summer salads you'll actually crave</td>\n",
       "      <td>6 summer salads you will actually crave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>20 suede pieces you'll want to wear all spring</td>\n",
       "      <td>20 suede pieces you will want to wear all spring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>you'll want to get inked after seeing this tat...</td>\n",
       "      <td>you will want to get inked after seeing this t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>the hilarious hipster classifieds you'll (prob...</td>\n",
       "      <td>the hilarious hipster classifieds you will (pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>apple fritter season is here, and so are the r...</td>\n",
       "      <td>apple fritter season is here, and so are the r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              headline  \\\n",
       "170              6 summer salads you'll actually crave   \n",
       "190     20 suede pieces you'll want to wear all spring   \n",
       "581  you'll want to get inked after seeing this tat...   \n",
       "621  the hilarious hipster classifieds you'll (prob...   \n",
       "721  apple fritter season is here, and so are the r...   \n",
       "\n",
       "                                                 clean  \n",
       "170            6 summer salads you will actually crave  \n",
       "190   20 suede pieces you will want to wear all spring  \n",
       "581  you will want to get inked after seeing this t...  \n",
       "621  the hilarious hipster classifieds you will (pr...  \n",
       "721  apple fritter season is here, and so are the r...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['headline'].str.contains(\"'ll\"), ['headline','clean']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove bad text:\n",
    "\n",
    "def remove_artifacts(s):\n",
    "    return re.sub('xe[0-9]+x[0-9]+x[0-9]+', '',s)\n",
    "\n",
    "data['clean'] = data['clean'].apply(lambda x: remove_artifacts(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>drug-resistant bacteria often lurk in children...</td>\n",
       "      <td>drug-resistant bacteria often lurk in children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>arkansas executes first inmate in 12 years</td>\n",
       "      <td>arkansas executes first inmate in 12 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>several women accuse progressive media executi...</td>\n",
       "      <td>several women accuse progressive media executi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>do you have to pay income taxes on social secu...</td>\n",
       "      <td>do you have to pay income taxes on social secu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>grab your wine boxes, because 'will &amp; grace' i...</td>\n",
       "      <td>grab your wine boxes, because 'will &amp; grace' i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              headline  \\\n",
       "24   drug-resistant bacteria often lurk in children...   \n",
       "220         arkansas executes first inmate in 12 years   \n",
       "356  several women accuse progressive media executi...   \n",
       "620  do you have to pay income taxes on social secu...   \n",
       "845  grab your wine boxes, because 'will & grace' i...   \n",
       "\n",
       "                                                 clean  \n",
       "24   drug-resistant bacteria often lurk in children...  \n",
       "220         arkansas executes first inmate in 12 years  \n",
       "356  several women accuse progressive media executi...  \n",
       "620  do you have to pay income taxes on social secu...  \n",
       "845  grab your wine boxes, because 'will & grace' i...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['headline'].str.contains(\"xe\"), ['headline','clean']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove possessive 's\n",
    "data['clean'] = data['clean'].apply(lambda x: re.sub(\"\\'s$\", '', x))\n",
    "data['clean'] = data['clean'].apply(lambda x: re.sub(\"\\'s \", ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>advancing the world's women</td>\n",
       "      <td>advancing the world women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>friday's morning email: inside trump's presser...</td>\n",
       "      <td>friday morning email: inside trump presser for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>actually, cnn's jeffrey lord has been 'indefen...</td>\n",
       "      <td>actually, cnn jeffrey lord has been 'indefensi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bloomberg's program to build better cities jus...</td>\n",
       "      <td>bloomberg program to build better cities just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>gillian jacobs on what it's like to kiss adam ...</td>\n",
       "      <td>gillian jacobs on what it like to kiss adam brody</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             headline  \\\n",
       "3                         advancing the world's women   \n",
       "6   friday's morning email: inside trump's presser...   \n",
       "10  actually, cnn's jeffrey lord has been 'indefen...   \n",
       "12  bloomberg's program to build better cities jus...   \n",
       "21  gillian jacobs on what it's like to kiss adam ...   \n",
       "\n",
       "                                                clean  \n",
       "3                           advancing the world women  \n",
       "6   friday morning email: inside trump presser for...  \n",
       "10  actually, cnn jeffrey lord has been 'indefensi...  \n",
       "12  bloomberg program to build better cities just ...  \n",
       "21  gillian jacobs on what it like to kiss adam brody  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['headline'].str.contains(\"'s\"), ['headline','clean']][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove Special Characters\n",
    "data['clean'] = data['clean'].apply(lambda x: re.sub('[^A-Za-z0-9 ]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Trim Extra White Space\n",
    "data['clean'] = data['clean'].apply(lambda x: re.sub('\\s{2,}', ' ', x))\n",
    "data['clean'] = data['clean'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246667, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guardian_length = data[data.source == 'guardian'].shape[0]\n",
    "\n",
    "Huff = data[data.source == 'Huffington Post']\n",
    "ABC = data[data.source == 'ABC Australia']\n",
    "\n",
    "other_sources = data[(data.source != 'Huffington Post') & (data.source != 'ABC Australia')]\n",
    "\n",
    "Huff = Huff.sample(n=guardian_length)\n",
    "ABC = ABC.sample(n=guardian_length)\n",
    "\n",
    "data_sources = [Huff, ABC, other_sources]\n",
    "rebalanced = pd.concat(data_sources)\n",
    "\n",
    "rebalanced = rebalanced.loc[~rebalanced['clean'].isnull()]\n",
    "rebalanced = rebalanced.drop_duplicates()\n",
    "rebalanced.reset_index(drop=True, inplace = True)\n",
    "\n",
    "rebalanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalanced[['source', 'year', 'clean', 'is_sarcastic']].to_csv('../data/combined_clean_v10.csv', index = False, sep = \"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profanity and Source Specific Information Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../references/profanity_wordlist.txt','r')\n",
    "words = []\n",
    "for line in file:\n",
    "    stripped_line = line.strip()\n",
    "    words.append(stripped_line)\n",
    "\n",
    "fake_swears = ['anus','aryan','bdsm','bondage','breasts','crap','drunk','enlargement','erection','erotic',\n",
    "              'erotism','facial','fat','gay','gays','gigolo','glans','god','hell', 'hemp','heroin','herpes',\n",
    "              'hitler','hiv','hookah','hooker','hooter','hooters','hump','humped','humping','incest','jerk',\n",
    "              'junkie','junky','kill','klan','kraut','LEN','lesbians','lesbos','loin','loins','lube','lust',\n",
    "              'lusting','lusty','masochist','masturbate','masturbating','masturbation','menses','menstruate',\n",
    "               'menstruation','meth','molest','moron','muff', 'murder','naked','napalm','nazi','nazism','nipple',\n",
    "               'nipples','nude', 'nudes','opiate','opium','oral','orally','organ','ovary','ovum','ovums','pawn',\n",
    "               'pcp','pedo','pedophile','pedophilia','pedophiliac','penetrate','penetration','perversion','peyote',\n",
    "               'phallic','pimp', 'playboy','pollock','porn', 'pornography','pornos','pot','potty','prick','pricks',\n",
    "               'prostitute','prude','punky','queer','quicky','racy','rape','raped','raper','raping','rapist',\n",
    "               'raunch','revue','rum','rump','sadism','sadist','sandbar','scantilly','screw','screwed','screwing',\n",
    "               'scum','seaman','seamen','seduce', 'sex','sexual','slave','sleaze','sleazy','slope','smut','smutty',\n",
    "               'snatch','sniper','snuff','sodom', 'souse', 'steamy', 'stoned', 'strip', 'strip club', 'stripclub',\n",
    "               'stroke','stupied','suck','sucked','sucking','tampon','tawdry','thrust', 'toke','tramp', 'trashy',\n",
    "               'ugly','unwed','urinal','urine','uterus','uzi','valium','viagra','virgin','vixen','vodka','vomit',\n",
    "               'voyeur','vulgar','weed','weirdo','wench','yury']\n",
    "\n",
    "real_swears = list()\n",
    "for word in words:\n",
    "    if word not in fake_swears:\n",
    "        real_swears.append(word)\n",
    "\n",
    "\n",
    "def profanity_remove(text):\n",
    "    if type(text)!='str':\n",
    "        text = str(text)\n",
    "    initial_words = text.split()\n",
    "    new_words = list(filter(lambda w: w not in real_swears, initial_words))\n",
    "    new_text = ' '.join(new_words)\n",
    "    return new_text\n",
    "\n",
    "data['clean'] = data['clean'].apply(lambda x:profanity_remove(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['huffpollster', 'huffpost', 'gps guide', 'on the looney front', 'morning email','talktome', \n",
    "         'daily meditation', 'sunday roundup','planet america', 'talking pictures','sunday session','tasmania', \n",
    "        'perth', 'png','adelaide', 'wa', 'afl', 'queensland', 'nt','canberra','nsw', 'aboriginal', 'sa','hobart', \n",
    "         'qld','melbourne', 'brisbane', 'sydney', 'socceroos',  'nrl','bushfire', 'tasmanian', 'just game', \n",
    "        'rba', 'tas', 'matildas', 'qna', 'nz', 'nbn', 'sydneys', 'cairns','australians', 'newcastle',\n",
    "         'tasmanias', 'asx', 'ndis', 'gold coast', 'aflw', 'townsville', 'adelaides', 'aussie',\n",
    "         'news brief', 'in pictures', 'the guardian view', 'weatherwatch','track of the day', 'dear therapist', \n",
    "         'photos of the week', 'the family weekly','the books briefing', 'america by air', 'gaffe track',\n",
    "         'todays news', 'poem of the day','trump time capsule', 'poem of the week', 'photo contest',\n",
    "         'time capsule', 'education stories','big stories','onion', 'things to know','strongsideweakside', \n",
    "         'the onions', 'your horoscopes','things to watch', 'week in pictures', 'fox news poll', 'fox nation', \n",
    "        'week in politics', 'sunday politics','nprreads','troll watch']\n",
    "\n",
    "\n",
    "mask = data['clean'].str.contains(r'\\b(?:{})\\b'.format('|'.join(words)))\n",
    "data = data[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trim Extra White Space\n",
    "data['clean'] = data['clean'].apply(lambda x: re.sub('\\s{2,}', ' ', x))\n",
    "data['clean'] = data['clean'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241413, 6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guardian_length = data[data.source == 'guardian'].shape[0]\n",
    "\n",
    "Huff = data[data.source == 'Huffington Post']\n",
    "ABC = data[data.source == 'ABC Australia']\n",
    "\n",
    "other_sources = data[(data.source != 'Huffington Post') & (data.source != 'ABC Australia')]\n",
    "\n",
    "Huff = Huff.sample(n=guardian_length)\n",
    "ABC = ABC.sample(n=guardian_length)\n",
    "\n",
    "data_sources = [Huff, ABC, other_sources]\n",
    "rebalanced = pd.concat(data_sources)\n",
    "\n",
    "rebalanced = rebalanced.loc[~rebalanced['clean'].isnull()]\n",
    "rebalanced = rebalanced.drop_duplicates()\n",
    "rebalanced.reset_index(drop=True, inplace = True)\n",
    "\n",
    "rebalanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalanced[['source', 'year', 'clean', 'is_sarcastic']].to_csv('../data/combined_clean_v10_nonsource.csv', index = False, sep = \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
